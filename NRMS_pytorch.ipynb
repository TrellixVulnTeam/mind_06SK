{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "name": "NRMS_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qrEc27EYIlOm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs229-mind/mind/blob/main/NRMS_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6OlTBsZIlOa"
      },
      "source": [
        "# NRMS: Neural News Recommendation with Multi-Head Self-Attention\n",
        "The following documents are copied from MIND public document.\n",
        "\n",
        "[NRMS](https://wuch15.github.io/paper/EMNLP2019-NRMS.pdf) is a neural news recommendation approach with multi-head selfattention. The core of NRMS is a news encoder and a user encoder. In the newsencoder, a multi-head self-attentions is used to learn news representations from news titles by modeling the interactions between words. In the user encoder, we learn representations of users from their browsed news and use multihead self-attention to capture the relatedness between the news. Besides, we apply additive\n",
        "attention to learn more informative news and user \n",
        "representations by selecting important words and news.\n",
        "\n",
        "## Properties of NRMS:\n",
        "- NRMS is a content-based neural news recommendation approach.\n",
        "- It uses multi-self attention to learn news representations by modeling the iteractions between words and learn user representations by capturing the relationship between user browsed news.\n",
        "- NRMS uses additive attentions to learn informative news and user representations by selecting important words and news.\n",
        "\n",
        "## Data format:\n",
        "For quicker training and evaluaiton, we sample MINDdemo dataset of 5k users from [MIND small dataset](https://msnews.github.io/). The MINDdemo dataset has the same file format as MINDsmall and MINDlarge. If you want to try experiments on MINDsmall and MINDlarge, please change the dowload source. Select the MIND_type parameter from ['large', 'small', 'demo'] to choose dataset.\n",
        " \n",
        "**MINDdemo_train** is used for training, and **MINDdemo_dev** is used for evaluation. Training data and evaluation data are composed of a news file and a behaviors file. You can find more detailed data description in [MIND repo](https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md)\n",
        "\n",
        "### news data\n",
        "This file contains news information including newsid, category, subcatgory, news title, news abstarct, news url and entities in news title, entities in news abstarct.\n",
        "One simple example: <br>\n",
        "\n",
        "`N46466\tlifestyle\tlifestyleroyals\tThe Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By\tShop the notebooks, jackets, and more that the royals can't live without.\thttps://www.msn.com/en-us/lifestyle/lifestyleroyals/the-brands-queen-elizabeth,-prince-charles,-and-prince-philip-swear-by/ss-AAGH0ET?ocid=chopendata\t[{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]\t[]`\n",
        "<br>\n",
        "\n",
        "In general, each line in data file represents information of one piece of news: <br>\n",
        "\n",
        "`[News ID] [Category] [Subcategory] [News Title] [News Abstrct] [News Url] [Entities in News Title] [Entities in News Abstract] ...`\n",
        "\n",
        "<br>\n",
        "\n",
        "We generate a word_dict file to tranform words in news title to word indexes, and a embedding matrix is initted from pretrained glove embeddings.\n",
        "\n",
        "### behaviors data\n",
        "One simple example: <br>\n",
        "`1\tU82271\t11/11/2019 3:28:58 PM\tN3130 N11621 N12917 N4574 N12140 N9748\tN13390-0 N7180-0 N20785-0 N6937-0 N15776-0 N25810-0 N20820-0 N6885-0 N27294-0 N18835-0 N16945-0 N7410-0 N23967-0 N22679-0 N20532-0 N26651-0 N22078-0 N4098-0 N16473-0 N13841-0 N15660-0 N25787-0 N2315-0 N1615-0 N9087-0 N23880-0 N3600-0 N24479-0 N22882-0 N26308-0 N13594-0 N2220-0 N28356-0 N17083-0 N21415-0 N18671-0 N9440-0 N17759-0 N10861-0 N21830-0 N8064-0 N5675-0 N15037-0 N26154-0 N15368-1 N481-0 N3256-0 N20663-0 N23940-0 N7654-0 N10729-0 N7090-0 N23596-0 N15901-0 N16348-0 N13645-0 N8124-0 N20094-0 N27774-0 N23011-0 N14832-0 N15971-0 N27729-0 N2167-0 N11186-0 N18390-0 N21328-0 N10992-0 N20122-0 N1958-0 N2004-0 N26156-0 N17632-0 N26146-0 N17322-0 N18403-0 N17397-0 N18215-0 N14475-0 N9781-0 N17958-0 N3370-0 N1127-0 N15525-0 N12657-0 N10537-0 N18224-0`\n",
        "<br>\n",
        "\n",
        "In general, each line in data file represents one instance of an impression. The format is like: <br>\n",
        "\n",
        "`[Impression ID] [User ID] [Impression Time] [User Click History] [Impression News]`\n",
        "\n",
        "<br>\n",
        "\n",
        "User Click History is the user historical clicked news before Impression Time. Impression News is the displayed news in an impression, which format is:<br>\n",
        "\n",
        "`[News ID 1]-[label1] ... [News ID n]-[labeln]`\n",
        "\n",
        "<br>\n",
        "Label represents whether the news is clicked by the user. All information of news in User Click History and Impression News can be found in news data file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpv4qh8OzDCQ"
      },
      "source": [
        "## Self Defined Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehIB1EQtcqV4"
      },
      "source": [
        "\n",
        "class AdditiveAttention(nn.Module):\n",
        "    ''' AttentionPooling used to weighted aggregate news vectors\n",
        "    Arg: \n",
        "        d_h: the last dimension of input\n",
        "    '''\n",
        "    def __init__(self, d_h, hidden_size=200):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.att_fc1 = nn.Linear(d_h, hidden_size)\n",
        "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: batch_size, candidate_size, candidate_vector_dim\n",
        "            attn_mask: batch_size, candidate_size\n",
        "        Returns:\n",
        "            (shape) batch_size, candidate_vector_dim\n",
        "        \"\"\"\n",
        "        bz = x.shape[0]\n",
        "        e = torch.tanh(self.att_fc1(x))\n",
        "        #e = nn.Tanh()(e)\n",
        "        alpha = self.att_fc2(e)\n",
        "\n",
        "        alpha = torch.exp(alpha)\n",
        "        if attn_mask is not None:\n",
        "            alpha = alpha * attn_mask.unsqueeze(2)\n",
        "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "        x = torch.bmm(x.permute(0, 2, 1), alpha)\n",
        "        x = torch.reshape(x, (bz, -1))  # (bz, 400)\n",
        "        return x\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask=None):\n",
        "        #       [bz, 20, seq_len, 20] x [bz, 20, 20, seq_len] -> [bz, 20, seq_len, seq_len]\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
        "        scores = torch.exp(scores)\n",
        "        if attn_mask is not None:\n",
        "            scores = scores * attn_mask\n",
        "        attn = scores / (torch.sum(scores, dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        #       [bz, 20, seq_len, seq_len] x [bz, 20, seq_len, 20] -> [bz, 20, seq_len, 20]\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_k, d_v, enable_gpu):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model  # 300\n",
        "        self.n_heads = n_heads  # 20\n",
        "        self.d_k = d_k  # 20\n",
        "        self.d_v = d_v  # 20\n",
        "        self.enable_gpu = enable_gpu\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)  # 300, 400\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)  # 300, 400\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)  # 300, 400\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        #       Q, K, V: [bz, seq_len, 300] -> W -> [bz, seq_len, 400]-> q_s: [bz, 20, seq_len, 20]\n",
        "        batch_size, seq_len, _ = Q.shape\n",
        "\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads,\n",
        "                               self.d_k).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads,\n",
        "                               self.d_k).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads,\n",
        "                               self.d_v).transpose(1, 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).expand(batch_size, seq_len, seq_len) #  [bz, seq_len, seq_len]\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [bz, 20, seq_len, seq_len]\n",
        "\n",
        "        context, attn = ScaledDotProductAttention(self.d_k)(\n",
        "            q_s, k_s, v_s, mask)  # [bz, 20, seq_len, 20]\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.n_heads * self.d_v)  # [bz, seq_len, 400]\n",
        "        #         output = self.fc(context)\n",
        "        return context  #self.layer_norm(output + residual)\n",
        "\n",
        "\n",
        "class TextEmbedding(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_model,\n",
        "                 dropout=0.0,\n",
        "                 layers=None,\n",
        "                 enable_gpu=True):\n",
        "        super(TextEmbedding, self).__init__()\n",
        "        #self.word_embedding = word_embedding\n",
        "        self.bert_model = bert_model  ## output embeddings from pretrained model\n",
        "        self.layers = [-4, -3, -2, -1] if layers is None else layers ## Use last 4 layers hidden state average to represent the embedding\n",
        "        self.Dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, text, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text: Tensor(batch_size) * num_words_text * embedding_dim\n",
        "        Returns:\n",
        "            (shape) batch_size, word_embedding_dim\n",
        "        \"\"\"\n",
        "        # batch_size, num_words_text\n",
        "        batch_size, num_words = text.shape\n",
        "        num_words = num_words // 3\n",
        "        text_ids = torch.narrow(text, 1, 0, num_words)\n",
        "        text_type = torch.narrow(text, 1, num_words, num_words)\n",
        "        text_attmask = torch.narrow(text, 1, num_words*2, num_words)\n",
        "        states = self.bert_model(text_ids, text_type, text_attmask).hidden_states\n",
        "        word_emb = states[-1] #torch.stack([states[i] for i in self.layers]).sum(0).squeeze()\n",
        "        return self.Dropout(word_emb)\n",
        "\n",
        "\n",
        "# class ElementEncoder(torch.nn.Module):\n",
        "#     def __init__(self, num_elements, embedding_dim, enable_gpu=True):\n",
        "#         super(ElementEncoder, self).__init__()\n",
        "#         self.enable_gpu = enable_gpu\n",
        "#         self.embedding = nn.Embedding(num_elements,\n",
        "#                                       embedding_dim,\n",
        "#                                       padding_idx=0)\n",
        "#\n",
        "#     def forward(self, element):\n",
        "#         # batch_size, embedding_dim\n",
        "#         element_vector = self.embedding(\n",
        "#             (element.cuda() if self.enable_gpu else element).long())\n",
        "#         return element_vector\n",
        "\n",
        "\n",
        "class NewsEncoder(torch.nn.Module):\n",
        "    def __init__(self, args, bert_model, category_dict_size,\n",
        "                 domain_dict_size, subcategory_dict_size, enable_gpu=True):\n",
        "        super(NewsEncoder, self).__init__()\n",
        "        self.args = args\n",
        "        self.attributes2length = {\n",
        "            'title': args.num_words_title * 3,\n",
        "            'abstract': args.num_words_abstract * 3,\n",
        "            'body': args.num_words_body * 3,\n",
        "            'category': 1,\n",
        "            'domain': 1,\n",
        "            'subcategory': 1\n",
        "        }\n",
        "        for key in list(self.attributes2length.keys()):\n",
        "            if key not in args.news_attributes:\n",
        "                self.attributes2length[key] = 0\n",
        "\n",
        "        self.attributes2start = {\n",
        "            key: sum(\n",
        "                list(self.attributes2length.values())\n",
        "                [:list(self.attributes2length.keys()).index(key)])\n",
        "            for key in self.attributes2length.keys()\n",
        "        }\n",
        "\n",
        "        assert len(args.news_attributes) > 0\n",
        "        text_encoders_candidates = ['title', 'abstract']\n",
        "\n",
        "        self.text_encoders = nn.ModuleDict({\n",
        "            'title':\n",
        "            TextEmbedding(bert_model, dropout=args.drop_rate)\n",
        "        })\n",
        "\n",
        "        self.newsname=[name for name in sorted(list(set(args.news_attributes) & set(text_encoders_candidates)))]\n",
        "\n",
        "        head_dim = args.news_dim // args.num_attention_heads\n",
        "\n",
        "        self.Multihead = MultiHeadAttention(args.word_emb_size,\n",
        "                                            args.num_attention_heads, head_dim,\n",
        "                                            head_dim, enable_gpu)  # args.news_dim = 400 = 20 * num_attention_heads\n",
        "        self.additiveatt = AdditiveAttention(args.news_dim, hidden_size=200)\n",
        "        self.Dropout = torch.nn.Dropout(args.drop_rate)\n",
        "\n",
        "    def forward(self, news):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        Returns:\n",
        "            (shape) batch_size, news_dim\n",
        "        \"\"\"\n",
        "        text_vectors = [\n",
        "            self.text_encoders['title'](\n",
        "                torch.narrow(news, 1, self.attributes2start[name],\n",
        "                             self.attributes2length[name]))\n",
        "            for name in self.newsname\n",
        "        ]\n",
        "\n",
        "        # batch_size, news_dim\n",
        "        input = text_vectors[0]\n",
        "        output = self.Multihead(input, input, input) # N * L * E\n",
        "        output = self.Dropout(output)\n",
        "        output = self.additiveatt(output) # N * E\n",
        "        return output\n",
        "\n",
        "class UserEncoder(torch.nn.Module):\n",
        "    def __init__(self, args, enable_gpu=True):\n",
        "        super(UserEncoder, self).__init__()\n",
        "        self.args = args\n",
        "        head_dim = args.news_dim // args.num_attention_heads\n",
        "\n",
        "        self.Multihead = MultiHeadAttention(args.news_dim,\n",
        "                                            args.num_attention_heads, head_dim,\n",
        "                                            head_dim, enable_gpu)\n",
        "        self.additiveatt = AdditiveAttention(args.news_dim, hidden_size=200)\n",
        "        self.Dropout = torch.nn.Dropout(args.drop_rate)\n",
        "\n",
        "\n",
        "    def forward(self, log_vec, log_mask=None):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            (shape) batch_size,  news_dim\n",
        "        \"\"\"\n",
        "        output = self.Multihead(log_vec, log_vec, log_vec)  # N * L * E\n",
        "        output = self.Dropout(output)\n",
        "        output = self.additiveatt(output)  # N * E\n",
        "        return output\n",
        "class TwoTowerModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    UniUM network.\n",
        "    Input 1 + K candidate news and a list of user clicked news, produce the click probability.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 args,\n",
        "                 bert_model,\n",
        "                 user_dict_size=0,\n",
        "                 category_dict_size=0,\n",
        "                 domain_dict_size=0,\n",
        "                 subcategory_dict_size=0):\n",
        "        super(TwoTowerModel, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "\n",
        "        self.news_encoder = NewsEncoder(args,\n",
        "                                        bert_model,\n",
        "                                        category_dict_size, \n",
        "                                        domain_dict_size,\n",
        "                                        subcategory_dict_size)\n",
        "        self.user_encoder = UserEncoder(args)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                log_ids,\n",
        "                log_mask,\n",
        "                targets=None,\n",
        "                compute_loss=True):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          click_probability: batch_size, 1 + K\n",
        "        \"\"\"\n",
        "        # input_ids: batch, 1+npratio, num_words\n",
        "        ids_length = input_ids.size(2)\n",
        "        input_ids = input_ids.view(-1, ids_length) # change to batch * (1+npratio), num_words\n",
        "        news_vec = self.news_encoder(input_ids)\n",
        "        news_vec = news_vec.view(-1, 1 + self.args.npratio, self.args.news_dim)\n",
        "\n",
        "        # batch_size, news_dim\n",
        "        log_ids = log_ids.view(-1, ids_length)\n",
        "        log_vec = self.news_encoder(log_ids)\n",
        "        log_vec = log_vec.view(-1, self.args.user_log_length,\n",
        "                               self.args.news_dim)\n",
        "\n",
        "        user_vector = self.user_encoder(log_vec, log_mask)\n",
        "\n",
        "        # batch_size, 2\n",
        "        score = torch.bmm(news_vec, user_vector.unsqueeze(-1)).squeeze(dim=-1)\n",
        "        if compute_loss:\n",
        "            loss = self.criterion(score, targets)\n",
        "            return loss, score\n",
        "        else:\n",
        "            return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPUewZz0xfzw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7twSCqkVIlOf"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Global settings and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnE90LpKTSUc",
        "outputId": "6592a481-ba19-4a0c-c4d3-413334a6d22d"
      },
      "source": [
        "#!ls gdrive/MyDrive\n",
        "!pip install transformers horovod"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: horovod in /usr/local/lib/python3.7/dist-packages (0.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from horovod) (5.4.8)\n",
            "Requirement already satisfied: cffi>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from horovod) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from horovod) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.4.0->horovod) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfNroSyvTlpI",
        "outputId": "4a537a60-bb49-43e0-cba6-45c8c30a373d"
      },
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "# %cd /content\n",
        "# !mkdir drive\n",
        "# %cd drive\n",
        "# !mkdir MyDrive\n",
        "# %cd ..\n",
        "# %cd ..\n",
        "# !google-drive-ocamlfuse \"/content/drive/MyDrive\"\n",
        "# import sys\n",
        "# path = '/content/drive/MyDrive'\n",
        "# sys.path.append(path)\n",
        "#data_path = '/content/drive/MyDrive'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "#define util module path\n",
        "path = '/content/gdrive/My Drive'\n",
        "sys.path.append(path)\n",
        "data_path = '/content/gdrive/My Drive'\n",
        "# import sys\n",
        "# import os\n",
        "# os.getcwd()\n",
        "%tensorflow_version 2.x\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "#import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from NRMS.dataloader import DataLoaderTrain, DataLoaderTest\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from NRMS.preprocess import read_news, read_news_bert, get_doc_input, get_doc_input_bert\n",
        "from NRMS.model_bert import ModelBert\n",
        "from NRMS.parameters import parse_args\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "import NRMS.utils as utils\n",
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import logging\n",
        "import datetime\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "import torch\n",
        "from torch import nn\n",
        "from NRMS.nrms import TwoTowerModel as TwoTower\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
        "print(\"Pytorch version: {}\".format(torch.__version__))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "System version: 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "Tensorflow version: 2.7.0\n",
            "Pytorch version: 1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmCI4jxzIlOj"
      },
      "source": [
        "## Prepare parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvGCosniAkhN",
        "outputId": "ce53d27b-c179-43eb-d894-e9842298168e"
      },
      "source": [
        "utils.setuplogger()\n",
        "args = parse_args()\n",
        "args.enable_hvd = False\n",
        "args.batch_size = 32\n",
        "args.news_dim = 400\n",
        "args.word_emb_size = 768\n",
        "data_path = '/content/gdrive/My Drive' # '/content/drive/MyDrive'\n",
        "train_dir = os.path.join(data_path, 'MINDsmall_train')\n",
        "train_news_file = os.path.join(train_dir, r'news.tsv')\n",
        "train_behaviors_file = os.path.join(train_dir, r'behaviors.tsv')\n",
        "\n",
        "valid_dir = os.path.join(data_path, 'MINDsmall_dev')\n",
        "valid_news_file = os.path.join(valid_dir, r'news.tsv')\n",
        "valid_behaviors_file = os.path.join(valid_dir, r'behaviors.tsv')\n",
        "\n",
        "test_dir = os.path.join(data_path, 'MINDsmall_test')\n",
        "test_news_file = os.path.join(test_dir, r'news.tsv')\n",
        "test_behaviors_file = os.path.join(test_dir, r'behaviors.tsv')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "config = AutoConfig.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\",config=config)\n",
        "bert_model = bert_model.cuda()\n",
        "\n",
        "finetuneset={\n",
        "'encoder.layer.10.attention.self.query.weight',\n",
        "'encoder.layer.10.attention.self.query.bias',\n",
        "'encoder.layer.10.attention.self.key.weight',\n",
        "'encoder.layer.10.attention.self.key.bias',\n",
        "'encoder.layer.10.attention.self.value.weight',\n",
        "'encoder.layer.10.attention.self.value.bias',\n",
        "'encoder.layer.10.attention.output.dense.weight',\n",
        "'encoder.layer.10.attention.output.dense.bias',\n",
        "'encoder.layer.10.attention.output.LayerNorm.weight',\n",
        "'encoder.layer.10.attention.output.LayerNorm.bias',\n",
        "'encoder.layer.10.intermediate.dense.weight',\n",
        "'encoder.layer.10.intermediate.dense.bias',\n",
        "'encoder.layer.10.output.dense.weight',\n",
        "'encoder.layer.10.output.dense.bias',\n",
        "'encoder.layer.10.output.LayerNorm.weight',\n",
        "'encoder.layer.10.output.LayerNorm.bias',\n",
        "'encoder.layer.11.attention.self.query.weight',\n",
        "'encoder.layer.11.attention.self.query.bias',\n",
        "'encoder.layer.11.attention.self.key.weight',\n",
        "'encoder.layer.11.attention.self.key.bias',\n",
        "'encoder.layer.11.attention.self.value.weight',\n",
        "'encoder.layer.11.attention.self.value.bias',\n",
        "'encoder.layer.11.attention.output.dense.weight',\n",
        "'encoder.layer.11.attention.output.dense.bias',\n",
        "'encoder.layer.11.attention.output.LayerNorm.weight',\n",
        "'encoder.layer.11.attention.output.LayerNorm.bias',\n",
        "'encoder.layer.11.intermediate.dense.weight',\n",
        "'encoder.layer.11.intermediate.dense.bias',\n",
        "'encoder.layer.11.output.dense.weight',\n",
        "'encoder.layer.11.output.dense.bias',\n",
        "'encoder.layer.11.output.LayerNorm.weight',\n",
        "'encoder.layer.11.output.LayerNorm.bias',\n",
        "'pooler.dense.weight',\n",
        "'pooler.dense.bias',\n",
        "'rel_pos_bias.weight',\n",
        "'classifier.weight',\n",
        "'classifier.bias'}\n",
        "for name,param in bert_model.named_parameters():\n",
        "    if name not in finetuneset:\n",
        "        param.requires_grad = False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2021-11-23 08:10:50,155] Namespace(batch_size=512, config_name='../Turing/unilm2-base-uncased-config.json', dataset='MIND', do_lower_case=True, drop_rate=0.2, embedding_source='random', enable_gpu=True, enable_hvd=True, epochs=4, f='/root/.local/share/jupyter/runtime/kernel-4dbea63a-6767-478a-9a52-15d882142a3f.json', filename_pat='behaviors*.tsv', filter_num=0, freeze_embedding=False, load_ckpt_name='epoch-1-40000.pt', log_steps=100, lr=0.0001, max_steps_per_epoch=200000, mode='test', model_dir='./model', model_name_or_path='../Turing/unilm2-base-uncased.bin', model_type='tnlrv3', news_attributes=['title'], news_dim=64, news_query_vector_dim=200, npratio=1, num_attention_heads=20, num_words_abstract=50, num_words_bing=26, num_words_body=50, num_words_title=24, num_words_uet=16, num_workers=2, padded_news_different_word_index=False, pretrain_news_encoder_path='.', process_bing=False, process_uet=False, root_data_dir='~/mind/', save_steps=500, shuffle_buffer_size=10000, test_dir='test', title_share_encoder=False, tokenizer_name='../Turing/unilm2-base-uncased-vocab.txt', train_dir='train', uet_agg_method='attention', use_padded_news_embedding=False, use_pretrain_news_encoder=False, user_log_length=50, user_log_mask=True, user_query_vector_dim=200, word_embedding_dim=768)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30cOYaDTIlOk"
      },
      "source": [
        "## Download and load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhDoORAnXg8p",
        "outputId": "e8a06b90-ebc3-4ff2-ba89-462575346d83"
      },
      "source": [
        "news, news_index, category_dict, domain_dict, subcategory_dict = read_news_bert(\n",
        "    train_news_file, \n",
        "    args,\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "news_title, news_title_type, news_title_attmask, \\\n",
        "news_abstract, news_abstract_type, news_abstract_attmask, \\\n",
        "news_body, news_body_type, news_body_attmask, \\\n",
        "news_category, news_domain, news_subcategory = get_doc_input_bert(\n",
        "    news, news_index, category_dict, domain_dict, subcategory_dict, args)\n",
        "\n",
        "word_dict = None\n",
        "#args.enable_hvd = False\n",
        "hvd_size, hvd_rank, hvd_local_rank = utils.init_hvd_cuda(\n",
        "        args.enable_hvd, args.enable_gpu)\n",
        "news_combined = np.concatenate([\n",
        "        x for x in\n",
        "        [news_title, news_title_type, news_title_attmask, \\\n",
        "            news_abstract, news_abstract_type, news_abstract_attmask, \\\n",
        "            news_body, news_body_type, news_body_attmask, \\\n",
        "            news_category, news_domain, news_subcategory]\n",
        "        if x is not None], axis=1)\n",
        "dataloader = DataLoaderTrain(\n",
        "        news_index=news_index,\n",
        "        news_combined=news_combined,\n",
        "        word_dict=word_dict,\n",
        "        data_dir=train_dir,\n",
        "        filename_pat=args.filename_pat,\n",
        "        args=args,\n",
        "        worker_size=hvd_size,\n",
        "        worker_rank=hvd_rank,\n",
        "        cuda_device_idx=hvd_local_rank,\n",
        "        enable_prefetch=True,\n",
        "        enable_shuffle=True,\n",
        "        enable_gpu=args.enable_gpu,\n",
        "    )\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "51282it [00:07, 6572.43it/s]\n",
            "100%|██████████| 51282/51282 [00:00<00:00, 87071.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4jf58arndKN"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKJqy9_s_zy4",
        "outputId": "9221f014-df82-4b54-e5c2-1261cd732a91"
      },
      "source": [
        "#from NRMS.nrms import TwoTowerModel as TwoTower\n",
        "model = TwoTower(args, bert_model, len(category_dict), len(domain_dict), len(subcategory_dict))\n",
        "args.optimizer = 'Adam'\n",
        "args.enable_lr_scheduler = False\n",
        "if args.enable_gpu:\n",
        "    model = model.cuda()\n",
        "if args.optimizer == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "elif args.optimizer == 'AdamW':\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, correct_bias=args.correct_bias)\n",
        "else:\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
        "if args.enable_lr_scheduler:\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "logging.info('Training...')\n",
        "LOSS, ACC = [], []\n",
        "args.epochs=1\n",
        "args.max_steps_per_epoch = 1000\n",
        "for ep in range(args.epochs):\n",
        "    loss = 0.0\n",
        "    accuracy = 0.0\n",
        "    tqdm_util = tqdm(enumerate(dataloader))\n",
        "    for cnt, (log_ids, log_mask, input_ids, targets) in tqdm_util:\n",
        "        if cnt > args.max_steps_per_epoch:\n",
        "            break\n",
        "\n",
        "        if args.enable_gpu:\n",
        "            log_ids = log_ids.cuda(non_blocking=True)\n",
        "            log_mask = log_mask.cuda(non_blocking=True)\n",
        "            input_ids = input_ids.cuda(non_blocking=True)\n",
        "            #user_ids = user_ids.cuda(non_blocking=True)\n",
        "            targets = targets.cuda(non_blocking=True)\n",
        "\n",
        "        bz_loss, y_hat = model(input_ids, log_ids, log_mask, targets)\n",
        "        # summary(model, [input_ids.shape, log_ids.shape, log_mask.shape, targets.shape], batch_size=16, device='cuda' if args.enable_gp else 'cpu')\n",
        "        optimizer.zero_grad()\n",
        "        bz_loss.backward()\n",
        "        optimizer.step()\n",
        "        if args.enable_lr_scheduler:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        loss += (bz_loss.data.float() - loss) / (cnt + 1)\n",
        "        accuracy += (utils.acc(targets, y_hat) - accuracy) / (cnt + 1)\n",
        "        if cnt % args.log_steps == 0:\n",
        "            LOSS.append(loss.data)\n",
        "            ACC.append(accuracy)\n",
        "            #logging.info(\n",
        "            tqdm_util.set_description(\n",
        "                '[{}] Ed: {}, train_loss: {:.5f}, acc: {:.5f}'.format(\n",
        "                    hvd_rank, cnt * args.batch_size, loss.data,\n",
        "                    accuracy))\n",
        "\n",
        "        # save model minibatch\n",
        "        #logging.info('[{}] Ed: {} {} {}'.format(hvd_rank, cnt, args.save_steps, cnt % args.save_steps))\n",
        "    logging.info('epoch: {} loss: {:.5f} accuracy {:.5f}'.format(ep + 1, loss, accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2021-11-23 08:11:04,362] Training...\n",
            "[INFO 2021-11-23 08:11:04,365] DataLoader __iter__()\n",
            "get_files: /content/gdrive/My Drive/MINDsmall_train, behaviors*.tsv"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "files: ['/content/gdrive/My Drive/MINDsmall_train/behaviors.tsv']\n",
            "[INFO 2021-11-23 08:11:04,377] worker_rank:0, worker_size:1, shuffle:True, seed:0, directory:/content/gdrive/My Drive/MINDsmall_train, files:['/content/gdrive/My Drive/MINDsmall_train/behaviors.tsv']\n",
            "[INFO 2021-11-23 08:11:04,380] data_paths: ['/content/gdrive/My Drive/MINDsmall_train/behaviors.tsv']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/mind_model/dataloader.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  user_feature_batch = torch.LongTensor(user_feature_batch).cuda()\n",
            "[0] Ed: 32000, train_loss: 0.68936, acc: 0.60568: : 1001it [25:28,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2021-11-23 08:36:32,669] epoch: 1 loss: 0.68936 accuracy 0.60568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn6chUx61-Zh"
      },
      "source": [
        "#args.log_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz18cYopA41q",
        "outputId": "542f86ec-7db2-4317-d21e-6b296838e5cd"
      },
      "source": [
        "#args.max_steps_per_epoch\n",
        "news, news_index, category_dict, domain_dict, subcategory_dict = read_news_bert(\n",
        "    valid_news_file, \n",
        "    args,\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "news_title, news_title_type, news_title_attmask, \\\n",
        "news_abstract, news_abstract_type, news_abstract_attmask, \\\n",
        "news_body, news_body_type, news_body_attmask, \\\n",
        "news_category, news_domain, news_subcategory = get_doc_input_bert(\n",
        "    news, news_index, category_dict, domain_dict, subcategory_dict, args)\n",
        "\n",
        "word_dict = None\n",
        "#args.enable_hvd = False\n",
        "hvd_size, hvd_rank, hvd_local_rank = utils.init_hvd_cuda(\n",
        "        args.enable_hvd, args.enable_gpu)\n",
        "news_combined = np.concatenate([\n",
        "        x for x in\n",
        "        [news_title, news_title_type, news_title_attmask, \\\n",
        "            news_abstract, news_abstract_type, news_abstract_attmask, \\\n",
        "            news_body, news_body_type, news_body_attmask, \\\n",
        "            news_category, news_domain, news_subcategory]\n",
        "        if x is not None], axis=1)\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "def news_collate_fn(arr):\n",
        "    arr = torch.LongTensor(arr)\n",
        "    return arr\n",
        "\n",
        "news_dataset = NewsDataset(news_combined)\n",
        "news_dataloader = DataLoader(news_dataset,\n",
        "                            batch_size=args.batch_size * 4,\n",
        "                            num_workers=args.num_workers,\n",
        "                            collate_fn=news_collate_fn)\n",
        "\n",
        "news_scoring = []\n",
        "with torch.no_grad():\n",
        "    for input_ids in tqdm(news_dataloader):\n",
        "        if args.enable_gpu:\n",
        "            input_ids = input_ids.cuda()\n",
        "        news_vec = model.news_encoder(input_ids)\n",
        "        news_vec = news_vec.to(torch.device(\"cpu\")).detach().numpy()\n",
        "        news_scoring.extend(news_vec)\n",
        "\n",
        "news_scoring = np.array(news_scoring)    \n",
        "\n",
        "dev_dataloader = DataLoaderTest(\n",
        "        news_index=news_index,\n",
        "        news_scoring=news_scoring,\n",
        "        word_dict=word_dict,\n",
        "        data_dir=valid_dir,\n",
        "        filename_pat=args.filename_pat,\n",
        "        args=args,\n",
        "        worker_size=hvd_size,\n",
        "        worker_rank=hvd_rank,\n",
        "        cuda_device_idx=hvd_local_rank,\n",
        "        enable_prefetch=True,\n",
        "        enable_shuffle=True,\n",
        "        enable_gpu=args.enable_gpu,\n",
        "    )\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "42416it [00:06, 6952.22it/s]\n",
            "100%|██████████| 42416/42416 [00:00<00:00, 64988.56it/s]\n",
            "100%|██████████| 332/332 [00:34<00:00,  9.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "07Np44B7rOz2",
        "outputId": "96bf77ce-adb1-46e6-d433-5554f3146817"
      },
      "source": [
        "from NRMS.metrics import roc_auc_score, ndcg_score, mrr_score, ctr_score\n",
        "import csv\n",
        "import datetime\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "AUC, MRR, nDCG5, nDCG10, SCORE = [], [], [], [], []\n",
        "count = 0\n",
        "outfile = os.path.join(valid_dir, \"prediction_{}_{}.tsv\".format(datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"), hvd_local_rank))\n",
        "\n",
        "\n",
        "def get_mean(arr):\n",
        "    return [np.array(i).mean() for i in arr]\n",
        "\n",
        "def write_score(SCORE, outfile):\n",
        "    # format the score: ImpressionID [Rank-of-News1,Rank-of-News2,...,Rank-of-NewsN]\n",
        "    for score in tqdm(SCORE):\n",
        "        argsort = np.argsort(-score[1])\n",
        "        ranks = np.empty_like(argsort)\n",
        "        ranks[argsort] = np.arange(len(score[1]))\n",
        "        score[1] = (ranks + 1).tolist()\n",
        "\n",
        "    # save the prediction result\n",
        "    def write_tsv(score):\n",
        "        with open(outfile, 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerows(score)\n",
        "    write_tsv(SCORE)\n",
        "tqdm_util_eval = tqdm(enumerate(dev_dataloader))\n",
        "def print_metrics(hvd_local_rank, cnt, x):\n",
        "    tqdm_util.set_description(\"[{}] Ed: {}: {}\".format(hvd_local_rank, cnt, \\\n",
        "        '\\t'.join([\"{:0.2f}\".format(i * 100) for i in x])))\n",
        "    \n",
        "with torch.no_grad():    \n",
        "    for cnt, (impression_ids, log_vecs, log_mask, news_vecs, news_bias, labels) in tqdm_util_eval:\n",
        "        #if cnt > 0:\n",
        "        #    break\n",
        "        #(log_ids, log_mask, input_ids, targets) = data_minibatch\n",
        "        count = cnt\n",
        "\n",
        "        if args.enable_gpu:\n",
        "            #user_ids = user_ids.cuda(non_blocking=True)\n",
        "            log_vecs = log_vecs.cuda(non_blocking=True)\n",
        "            log_mask = log_mask.cuda(non_blocking=True)\n",
        "        # print(impression_ids)\n",
        "        # print(log_vecs.size())\n",
        "        # print(log_mask.size())\n",
        "        # print(len(news_vecs))\n",
        "        # print(len(news_bias))\n",
        "        # print(len(labels))\n",
        "        user_vecs = model.user_encoder(log_vecs, log_mask).to(torch.device(\"cpu\")).detach().numpy()\n",
        "\n",
        "        for impression_id, user_vec, news_vec, bias, label in zip(\n",
        "                impression_ids, user_vecs, news_vecs, news_bias, labels):\n",
        "\n",
        "            #if label.mean() == 0 or label.mean() == 1:\n",
        "            #    continue\n",
        "\n",
        "            score = np.dot(\n",
        "                news_vec, user_vec\n",
        "            )\n",
        "\n",
        "            # label is -1 is for test set and prediction only\n",
        "            if(np.all(label == -1)):\n",
        "                SCORE.append([impression_id, score])\n",
        "                continue\n",
        "\n",
        "            auc = roc_auc_score(label, score)\n",
        "            mrr = mrr_score(label, score)\n",
        "            ndcg5 = ndcg_score(label, score, k=5)\n",
        "            ndcg10 = ndcg_score(label, score, k=10)\n",
        "\n",
        "            AUC.append(auc)\n",
        "            MRR.append(mrr)\n",
        "            nDCG5.append(ndcg5)\n",
        "            nDCG10.append(ndcg10)\n",
        "\n",
        "        if cnt % args.log_steps == 0:\n",
        "            # print_metrics(hvd_rank, cnt * args.batch_size, [1.0])\n",
        "            print_metrics(hvd_rank, cnt * args.batch_size, get_mean([AUC, MRR, nDCG5,  nDCG10]))\n",
        "\n",
        "        if cnt % args.save_steps == 0:\n",
        "            if len(SCORE) > 0:\n",
        "                tqdm_util.set_description(\"[{}] Ed: {}: saving {} lines to {}\".format(hvd_local_rank, cnt, len(SCORE), outfile))\n",
        "                write_score(SCORE, outfile)\n",
        "                SCORE = []\n",
        "\n",
        "# stop scoring\n",
        "logging.info(\"Stop scoring\")\n",
        "dataloader.join()\n",
        "\n",
        "# save the last batch of scores\n",
        "if len(SCORE) > 0:\n",
        "    logging.info(\"[{}] Ed: {}: saving {} lines to {}\".format(hvd_local_rank, cnt, len(SCORE), outfile))\n",
        "    write_score(SCORE, outfile)\n",
        "    SCORE = []\n",
        "\n",
        "# print and save metrics\n",
        "logging.info(\"Print final metrics\")\n",
        "print_metrics(hvd_rank, count * args.batch_size, get_mean([AUC, MRR, nDCG5,  nDCG10]))\n",
        "\n",
        "logging.info(f\"Time taken: {time.time() - start_time}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO 2021-11-23 10:19:48,398] DataLoader __iter__()\n",
            "[INFO 2021-11-23 10:19:48,402] shut down pool.\n",
            "get_files: /content/gdrive/My Drive/MINDsmall_dev, behaviors*.tsv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "files: ['/content/gdrive/My Drive/MINDsmall_dev/behaviors.tsv']\n",
            "[INFO 2021-11-23 10:19:48,417] worker_rank:0, worker_size:1, shuffle:True, seed:10, directory:/content/gdrive/My Drive/MINDsmall_dev, files:['/content/gdrive/My Drive/MINDsmall_dev/behaviors.tsv']\n",
            "[INFO 2021-11-23 10:19:48,418] visible_devices:[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "[INFO 2021-11-23 10:19:48,423] [StreamReader] path_len:1, paths: ['/content/gdrive/My Drive/MINDsmall_dev/behaviors.tsv']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2285it [08:00, 14.37it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EHvG72VYESJr",
        "outputId": "20dddd63-61a5-491e-a05b-9333c6d0b6a0"
      },
      "source": [
        "#outfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/MINDsmall_dev/prediction_20211123101247_0.tsv'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "2WXOTRnJ7now",
        "outputId": "354918bf-9845-4ddd-a6c9-d9ad8b278971"
      },
      "source": [
        "#score = torch.bmm(news_vec, user_vecs.expand_dims(-1)).squeeze(dim=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9f86d6972d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'expand_dims'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO8Gn7DusMtR"
      },
      "source": [
        "\n",
        "loss = criterion(score, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7QWzKi2hvcj",
        "outputId": "07bba777-ef4e-4fc8-c9bf-0de66f636f6b"
      },
      "source": [
        "loss.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(41.4614, device='cuda:0')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrEc27EYIlOm"
      },
      "source": [
        "## Train the NRMS model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lx-vRMwIlOn"
      },
      "source": [
        "iterator = MINDIterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmWQeNjGIlOn",
        "scrolled": true,
        "outputId": "cf61d8c0-1685-4e68-f876-c0fe7d7f4422"
      },
      "source": [
        "model = NRMSModel(hparams, iterator, seed=seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fa1893290>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa17fe850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fa1893290>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa17fe850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fa1893290>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa17fe850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fabbf7410>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa06072d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fa1893290>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa17fe850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fa1893290>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa17fe850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fabbf7410>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa06072d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fa1893290>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa17fe850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fa1893290>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa17fe850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method SelfAttention.call of <recommenders.models.newsrec.models.layers.SelfAttention object at 0x7f5fa1893290>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method AttLayer2.call of <recommenders.models.newsrec.models.layers.AttLayer2 object at 0x7f5fa17fe850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ibGDQQX8Gqq"
      },
      "source": [
        "_= model.test_iterator.load_data_from_file(valid_news_file, valid_behaviors_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4yRjnxt9-9C",
        "outputId": "f57820dc-246e-46e1-cd63-bcacccce1650"
      },
      "source": [
        "dir(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_build_graph',\n",
              " '_build_newsencoder',\n",
              " '_build_nrms',\n",
              " '_build_userencoder',\n",
              " '_get_input_label_from_iter',\n",
              " '_get_loss',\n",
              " '_get_news_feature_from_iter',\n",
              " '_get_opt',\n",
              " '_get_pred',\n",
              " '_get_user_feature_from_iter',\n",
              " '_init_embedding',\n",
              " 'eval',\n",
              " 'fit',\n",
              " 'group_labels',\n",
              " 'hparams',\n",
              " 'loss',\n",
              " 'model',\n",
              " 'news',\n",
              " 'newsencoder',\n",
              " 'run_eval',\n",
              " 'run_fast_eval',\n",
              " 'run_news',\n",
              " 'run_slow_eval',\n",
              " 'run_user',\n",
              " 'scorer',\n",
              " 'seed',\n",
              " 'support_quick_scoring',\n",
              " 'test_iterator',\n",
              " 'train',\n",
              " 'train_iterator',\n",
              " 'train_optimizer',\n",
              " 'user',\n",
              " 'userencoder',\n",
              " 'word2vec_embedding']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oK87z5H9K7C",
        "outputId": "323b35d8-9bdf-4581-e1a7-edd4e8cbf311"
      },
      "source": [
        "dir(model.test_iterator)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ID_spliter',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_convert_data',\n",
              " '_convert_news_data',\n",
              " '_convert_user_data',\n",
              " 'batch_size',\n",
              " 'col_spliter',\n",
              " 'gen_feed_dict',\n",
              " 'his_size',\n",
              " 'init_behaviors',\n",
              " 'init_news',\n",
              " 'load_data_from_file',\n",
              " 'load_dict',\n",
              " 'load_impression_from_file',\n",
              " 'load_news_from_file',\n",
              " 'load_user_from_file',\n",
              " 'npratio',\n",
              " 'parser_one_line',\n",
              " 'title_size',\n",
              " 'uid2index',\n",
              " 'word_dict']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EH52g39-vS-"
      },
      "source": [
        "??model.test_iterator.load_data_from_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Y8NviY-28H"
      },
      "source": [
        "#news_file\n",
        "model.test_iterator.init_news(valid_news_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "rmjTMHPuIlOn",
        "outputId": "195dc2f3-c63c-40ca-d017-79d8a00ffd87"
      },
      "source": [
        "print(model.run_eval(valid_news_file, valid_behaviors_file))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3781it [01:42, 36.88it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a91b7bddd47e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_news_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_behaviors_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/gdrive/My Drive/recommenders/models/newsrec/models/base_model.py\u001b[0m in \u001b[0;36mrun_eval\u001b[0;34m(self, news_filename, behaviors_file)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_quick_scoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             _, group_labels, group_preds = self.run_fast_eval(\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0mnews_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehaviors_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             )\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/recommenders/models/newsrec/models/base_model.py\u001b[0m in \u001b[0;36mrun_fast_eval\u001b[0;34m(self, news_filename, behaviors_file)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_fast_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehaviors_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mnews_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0muser_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehaviors_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_vecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/recommenders/models/newsrec/models/base_model.py\u001b[0m in \u001b[0;36mrun_user\u001b[0;34m(self, news_filename, behaviors_file)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0muser_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         for batch_data_input in tqdm(\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_user_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehaviors_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         ):\n\u001b[1;32m    360\u001b[0m             \u001b[0muser_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/recommenders/models/newsrec/io/mind_iterator.py\u001b[0m in \u001b[0;36mload_user_from_file\u001b[0;34m(self, news_file, behavior_file)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"impr_indexes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_behaviors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehavior_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0muser_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/recommenders/models/newsrec/io/mind_iterator.py\u001b[0m in \u001b[0;36minit_behaviors\u001b[0;34m(self, behaviors_file)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mimpr_news\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnid2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0muindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/recommenders/models/newsrec/io/mind_iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mimpr_news\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnid2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0muindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHod9y7bIlOo",
        "outputId": "e703554e-cae0-4a9c-a538-08845e9e3ca0"
      },
      "source": [
        "%%time\n",
        "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "step 19720 , total_loss: 1.3392, data_loss: 1.2761: : 19719it [20:54:59,  3.82s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tp_Mf-KnIlOo",
        "outputId": "9e5a0da9-12a8-437d-c23e-f6e700bc32ce"
      },
      "source": [
        "%%time\n",
        "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
        "print(res_syn)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "586it [00:14, 39.70it/s]\n",
            "236it [04:17,  1.09s/it]\n",
            "7538it [00:01, 5112.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'group_auc': 0.608, 'mean_mrr': 0.2684, 'ndcg@5': 0.2872, 'ndcg@10': 0.3606}\n",
            "CPU times: user 8min 49s, sys: 7.46 s, total: 8min 57s\n",
            "Wall time: 4min 41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RPRQUGGyIlOp",
        "outputId": "04eec3e9-3335-489d-fe0a-cf25855a291a"
      },
      "source": [
        "sb.glue(\"res_syn\", res_syn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/scrapbook.scrap.json+json": {
              "data": {
                "group_auc": 0.608,
                "mean_mrr": 0.2684,
                "ndcg@10": 0.3606,
                "ndcg@5": 0.2872
              },
              "encoder": "json",
              "name": "res_syn",
              "version": 1
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_ahaftzIlOp"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK3ARC4--adM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1MtPU8AUIlOp"
      },
      "source": [
        "model_path = os.path.join(data_path, \"model\")\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "model.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoFwqhD-IlOp"
      },
      "source": [
        "## Output Predcition File\n",
        "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
        "\n",
        "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WKJ_60_ykHb"
      },
      "source": [
        "test_news_file = os.path.join(data_path, 'MINDlarge_test', r'news.tsv')\n",
        "test_behaviors_file = os.path.join(data_path, 'MINDlarge_test', r'behaviors.tsv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PWceBziIIlOp",
        "outputId": "77fea63b-5ad6-4428-a010-7aab31d21a48"
      },
      "source": [
        "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(test_news_file, test_behaviors_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "586it [00:14, 39.24it/s]\n",
            "236it [04:17,  1.09s/it]\n",
            "7538it [00:01, 5166.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vNTTOn7NIlOq",
        "outputId": "05cda72a-97e8-48a3-c85a-021ff1944318"
      },
      "source": [
        "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
        "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
        "        impr_index += 1\n",
        "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
        "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
        "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7538it [00:00, 35895.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9_xKcRYyIlOq"
      },
      "source": [
        "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
        "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWhPpQm6IlOq"
      },
      "source": [
        "## Reference\n",
        "\\[1\\] Wu et al. \"Neural News Recommendation with Multi-Head Self-Attention.\" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<br>\n",
        "\\[2\\] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>\n",
        "\\[3\\] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzYzsCwmU3lX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}